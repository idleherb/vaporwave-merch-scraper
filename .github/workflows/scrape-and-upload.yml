name: scrape-and-upload

on:
  schedule:
    # Run workflow every hour
    - cron: "0 * * * *"

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up Poetry
      uses: Gr1N/setup-poetry@v8
    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"
        cache: "poetry"
    - name: Install dependencies
      run: |
        poetry install
    - name: Run scraper
      run: |
        poetry run python -m scraper.main > bandcamp_merch.json
    - name: Store json results
      uses: actions/upload-artifact@v3
      with:
        name: scraping-results
        path: bandcamp_merch.json
        retention-days: 1

  upload-results:
    needs: scrape
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Create folder for synching
      run: |
        mkdir -p tmp
    - uses: actions/download-artifact@v3
      with:
        name: scraping-results
        path: tmp/
    - name: Upload scraping results to s3 bucket
      uses: jakejarvis/s3-sync-action@master
      with:
        args: --acl public-read
      env:
        AWS_S3_BUCKET: ${{ secrets.AWS_BUCKET }}
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        SOURCE_DIR: "tmp"
